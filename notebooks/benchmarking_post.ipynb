{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.038181988Z",
     "start_time": "2023-11-29T15:35:27.994989502Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, RocCurveDisplay, balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarking for all videos considered at once"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c50ed497b3c9ec44"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def compute_roc_auc(df, csv_file, proba='post_proba'):\n",
    "    y = df['label']\n",
    "    ŷ = df[proba]\n",
    "    result = roc_auc_score(y, ŷ)\n",
    "    csv_file['roc_auc'] = result\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.108204578Z",
     "start_time": "2023-11-29T15:35:28.001278895Z"
    }
   },
   "id": "9c5c1c8fe64341e6"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def get_confusion_matrix(true_labels: list, avg_model_labels: list, file_name: str, output_dir: str):\n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, avg_model_labels, labels=[0, 1])\n",
    "\n",
    "    # Calculate percentages\n",
    "    total = np.sum(conf_matrix)\n",
    "    percentage_matrix = (conf_matrix / total) * 100\n",
    "\n",
    "    # Create an annotated matrix that includes percentages\n",
    "    annot_matrix = np.array([[f\"{val}\\n({percentage:.2f}%)\" for val, percentage in zip(row, percentage_row)]\n",
    "                             for row, percentage_row in zip(conf_matrix, percentage_matrix)])\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=annot_matrix, fmt='',\n",
    "                xticklabels=[0, 1],\n",
    "                yticklabels=[0, 1],\n",
    "                cmap=\"Blues\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "    conf_matrix_path = os.path.join(output_dir, file_name)\n",
    "    plt.savefig(conf_matrix_path)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.111301727Z",
     "start_time": "2023-11-29T15:35:28.057480643Z"
    }
   },
   "id": "3cbde7cac7d1c7ea"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, pred_labels, label_name):\n",
    "    # Calculate metrics\n",
    "    accuracy = round(accuracy_score(true_labels, pred_labels) * 100, 2)  # TP + TN / n\n",
    "    precision = round(precision_score(true_labels, pred_labels, pos_label=1, average='binary') * 100,\n",
    "                      2)  # TP / TP + FP\n",
    "    recall = round(recall_score(true_labels, pred_labels, pos_label=1, average='binary') * 100, 2)  # TP / TP + FN\n",
    "    f1 = round(f1_score(true_labels, pred_labels, pos_label=1, average='binary') * 100,\n",
    "               2)  # 2 * [(recall * precision) / (recall + precision)]\n",
    "    balanced_accuracy = round(balanced_accuracy_score(true_labels, pred_labels) * 100, 2)\n",
    "\n",
    "    # Calculate percentages\n",
    "    correct_no = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true == pred and true == 0]) / true_labels.count(\n",
    "            0) * 100, 2)\n",
    "    incorrect_no = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true != pred and true == 0]) / true_labels.count(\n",
    "            0) * 100, 2)\n",
    "    correct_spur = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true == pred and true == 1]) / true_labels.count(\n",
    "            1) * 100, 2)\n",
    "    incorrect_spur = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true != pred and true == 1]) / true_labels.count(\n",
    "            1) * 100, 2)\n",
    "\n",
    "    return [label_name, balanced_accuracy, accuracy, precision, recall, f1, correct_no, incorrect_no, correct_spur, incorrect_spur]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.111408739Z",
     "start_time": "2023-11-29T15:35:28.057599716Z"
    }
   },
   "id": "27a262b3be18c9dc"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def evaluate_models(df, video_idx=None):\n",
    "    # Initialize DataFrame\n",
    "    if video_idx:\n",
    "        results_df = pd.DataFrame(\n",
    "            columns=['video_id', 'balanced_accuracy', 'accuracy', 'precision', 'recall', 'f1_score', 'correct_no', 'incorrect_no', 'correct_spur',\n",
    "                 'incorrect_spur'])\n",
    "    else:\n",
    "        results_df = pd.DataFrame(\n",
    "            columns=['k', 'balanced_accuracy', 'accuracy', 'precision', 'recall', 'f1_score', 'correct_no', 'incorrect_no', 'correct_spur',\n",
    "                 'incorrect_spur'])\n",
    "\n",
    "    # Extract true labels from df and filter None values\n",
    "    true_labels_k = [label for label in df['label'].tolist() if label is not None]\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col.startswith('post_pred'):\n",
    "            # Filter None values\n",
    "            model_labels_k = [label for label in df[col].tolist() if label is not None]\n",
    "\n",
    "            if len(true_labels_k) != len(model_labels_k):\n",
    "                print(f\"Skipping evaluation for {col} due to inconsistent label lengths after filtering None values.\")\n",
    "                continue\n",
    "\n",
    "            model_name = col.split('_')[1]\n",
    "            results_df.loc[len(results_df)] = calculate_metrics(true_labels_k, model_labels_k, video_idx)\n",
    "\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.111999193Z",
     "start_time": "2023-11-29T15:35:28.057643205Z"
    }
   },
   "id": "bb560e1865ce4a1"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def compute_auc_roc(output_file_path, results_frame_df, roc_curve_file_name, csv_file, save=True):\n",
    "    auc_roc = compute_roc_auc(results_frame_df, csv_file)\n",
    "    fpr, tpr, thresholds = compute_roc_curve(results_frame_df)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_roc).plot()\n",
    "    if save: roc_display.figure_.savefig(os.path.join(output_file_path, roc_curve_file_name))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.130359583Z",
     "start_time": "2023-11-29T15:35:28.105274345Z"
    }
   },
   "id": "14a7ca4f58e223f2"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def compute_roc_curve(df, proba='post_proba'):\n",
    "    #y = df['label'].map({'no': 0, 'spur': 1})\n",
    "    y = df['label']\n",
    "    ŷ = df[proba].astype(float)\n",
    "\n",
    "    result = roc_curve(y, ŷ, pos_label=1)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.131197276Z",
     "start_time": "2023-11-29T15:35:28.105369412Z"
    }
   },
   "id": "86b11620ba89fdda"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def create_benchmark(video_df, sub_dest_dir):\n",
    "    #get_confusion_matrix(true_labels=video_df['label'].tolist(), avg_model_labels=video_df['pred_avg'].tolist(),\n",
    "                         #file_name=f'confusion_matrix.png', output_dir=sub_dest_dir)\n",
    "    \n",
    "    results_frame_df = evaluate_models(video_df)\n",
    "    compute_auc_roc(sub_dest_dir, video_df, 'roc_curve.pdf', results_frame_df, save=False)\n",
    "    \n",
    "    output_file_path = os.path.join(sub_dest_dir, 'eval_output.csv')\n",
    "    results_frame_df.to_csv(output_file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.157520416Z",
     "start_time": "2023-11-29T15:35:28.111939425Z"
    }
   },
   "id": "edcc9e5ff2c5521f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e448b0d044038a8"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "source_dir = Path('../output/benchmarking/post_pro_sources')\n",
    "# Where to save\n",
    "results_dir = Path('../output/benchmarking/post_pro_result')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.157642391Z",
     "start_time": "2023-11-29T15:35:28.153384517Z"
    }
   },
   "id": "953c4326db08606d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Benchmarking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e658282af65274"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def process_directories(source_dir, results_dir):\n",
    "    for model_dir in source_dir.iterdir():\n",
    "        if model_dir.is_dir():\n",
    "            model_name = model_dir.name\n",
    "            all_results = []\n",
    "\n",
    "            # Lecture et traitement de chaque fichier CSV\n",
    "            for file in model_dir.glob(\"id_*.csv\"):\n",
    "                video_name = file.stem\n",
    "                temp_df = pd.read_csv(file)\n",
    "\n",
    "                # Vérification de l'existence des colonnes nécessaires\n",
    "                required_columns = ['frame', 'post_proba', 'post_pred', 'label']\n",
    "                if all(column in temp_df.columns for column in required_columns):\n",
    "                    temp_df = temp_df[required_columns]\n",
    "\n",
    "                    if model_name == 'nnunet' or model_name == 'vresnet':\n",
    "                        temp_df['label'] = temp_df['label'].replace({'no': 0, 'spur': 1})\n",
    "                    #     temp_df['post_pred'] = temp_df['post_pred'].replace({'no': 0, 'spur': 1})\n",
    "\n",
    "                    # Application des fonctions d'évaluation sur les données temporaires\n",
    "                    results_frame_df = evaluate_models(temp_df, video_name)\n",
    "                    auc_roc = compute_roc_auc(temp_df, results_frame_df)\n",
    "                    # Agrégation des résultats\n",
    "                    all_results.append(results_frame_df)\n",
    "                else:\n",
    "                    print(f\"Le fichier {file} ne contient pas toutes les colonnes requises.\")\n",
    "\n",
    "            # Concaténation des résultats de tous les fichiers\n",
    "            final_results_df = pd.concat(all_results)\n",
    "            numeric_cols = final_results_df.select_dtypes(include=['number']).columns\n",
    "            mean_values = final_results_df[numeric_cols].mean()\n",
    "            mean_values_list = ['average'] + mean_values.tolist()\n",
    "\n",
    "            mean_row = pd.DataFrame([mean_values_list], columns=final_results_df.columns)\n",
    "            final_results_df = final_results_df._append(mean_row, ignore_index=True)\n",
    "\n",
    "            # Sauvegarde du fichier CSV\n",
    "            final_results_df.to_csv(results_dir / f\"{model_name}.csv\", index=False)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:28.197574688Z",
     "start_time": "2023-11-29T15:35:28.197355060Z"
    }
   },
   "id": "90456d203643be57"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "final_results = process_directories(source_dir, results_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:29.162664509Z",
     "start_time": "2023-11-29T15:35:28.197495929Z"
    }
   },
   "id": "1c25b32f532872f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeddd7a2344374ab"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:35:29.174016421Z",
     "start_time": "2023-11-29T15:35:29.164095298Z"
    }
   },
   "id": "c77e743a08794dbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
