{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Collect Labels with Simple CNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "from io import StringIO\n",
    "from typing import List, Type\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from cnn import SimpleCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Verify data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_path = '../data/maps/mapping.csv'\n",
    "cwd = Path.cwd().parent\n",
    "video_dir = os.path.join(cwd, 'data', 'videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directories(mapping_path, video_dir_path):\n",
    "    # Read csv\n",
    "    df = pd.read_csv(mapping_path)\n",
    "    # Get count ofrows in csv\n",
    "    num_rows = len(df)\n",
    "    print(f\"How many rows in csv file: {num_rows}\")\n",
    "    \n",
    "    # Get count of directories in video folder\n",
    "    folder_names = [name for name in os.listdir(video_dir_path) if os.path.isdir(os.path.join(video_dir_path, name))]\n",
    "    num_folders = len(folder_names)\n",
    "    print(f\"How many directories in {video_dir_path}: {num_folders}\")\n",
    "    \n",
    "    # List missing directories\n",
    "    expected_folders = set(df['directory'])\n",
    "    actual_folders = set(folder_names)\n",
    "    missing_folders = expected_folders - actual_folders\n",
    "    \n",
    "    if missing_folders:\n",
    "        print(f\"Missing directories : {missing_folders}\")\n",
    "    else:\n",
    "        print(\"All directories are present.\")\n",
    "\n",
    "    return missing_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dir = check_directories(mapping_path, video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_missing_directories(missing_folders, destination_path, source_path = '/home/rob/Documents/data/selected_frames'):\n",
    "    copied_folders = 0\n",
    "    not_copied_folders = []\n",
    "    \n",
    "    for folder in missing_folders:\n",
    "        src_folder = os.path.join(source_path, folder)\n",
    "        dest_folder = os.path.join(destination_path, folder)\n",
    "        \n",
    "        if os.path.exists(src_folder):\n",
    "            shutil.copytree(src_folder, dest_folder)\n",
    "            copied_folders += 1\n",
    "        else:\n",
    "            not_copied_folders.append(folder)\n",
    "    \n",
    "    print(f\"Number of folders copied: {copied_folders}\")\n",
    "    if not_copied_folders:\n",
    "        print(f\"Folders that could not be copied: {not_copied_folders}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if missing_dir:\n",
    "    copy_missing_directories(missing_dir, destination_path=video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = []\n",
    "k = 5\n",
    "for i in range(k):\n",
    "    folder_path = f'weights/'\n",
    "    \n",
    "    # Liste tous les fichiers dans le dossier\n",
    "    all_files = os.listdir(folder_path)\n",
    "    \n",
    "    # Trouve le fichier qui termine par '__best.pt'\n",
    "    best_file = next((f for f in all_files), None)\n",
    "    \n",
    "    if best_file:\n",
    "        full_path = os.path.join(folder_path, best_file)\n",
    "        model_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_string(s):\n",
    "    return int(re.search(r'\\d+', s).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(mapping_path)\n",
    "\n",
    "# Verify that video names are unique\n",
    "assert(len(df.index) == len(df.directory.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    def __init__(self, frame, proba, label=None):\n",
    "        self.frame:int = frame\n",
    "        self.proba:float = proba\n",
    "        self.label:int = label\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Prediction(frame={self.frame}, proba={self.proba:.4f}, label={self.label})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionList():\n",
    "    def __init__(self, predictions, video_id, k):\n",
    "        self.predictions: List[Prediction] = predictions\n",
    "        self.video_id: str = video_id\n",
    "        self.k:int = k\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"PredictionList(video_id={self.video_id}, k={self.k}, predictions={self.predictions})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOfPredictionList(list):\n",
    "    def append(self, item: Type[PredictionList]):\n",
    "        if not isinstance(item, PredictionList):\n",
    "            raise ValueError(\"Item must be of type PredictionList\")\n",
    "        super().append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(video_id: str, current_vid_dir: Path) -> ListOfPredictionList:\n",
    "    result = ListOfPredictionList()\n",
    "\n",
    "    # For each model.pt\n",
    "    for i, model_path in enumerate(model_paths):\n",
    "        preds = PredictionList(k=i, video_id=video_id, predictions=[])\n",
    "        model = SimpleCNN().to(device)\n",
    "\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint)  \n",
    "        \n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        print(f'Start inference for k={i}')\n",
    "        \n",
    "        # Loop through all images\n",
    "        for img_name in tqdm(os.listdir(current_vid_dir), desc='infering...'):\n",
    "            if img_name.endswith('.png'):\n",
    "                # Load image\n",
    "                img_path = os.path.join(current_vid_dir, img_name)\n",
    "                img = Image.open(img_path)\n",
    "            \n",
    "                # Run inference\n",
    "                with torch.inference_mode():\n",
    "                    transformed_img = transform(img).unsqueeze(dim=0)  # Add a batch dimension\n",
    "                    pred = model(transformed_img.to(device))\n",
    "\n",
    "                    probabilities = torch.sigmoid(pred)\n",
    "                    predicted_label = (probabilities > threshold).int()\n",
    "\n",
    "                frame_number = extract_number_from_string(img_name.split('.')[0])\n",
    "                \n",
    "                # Save Inference Result\n",
    "                pred = Prediction(proba=probabilities.item(), frame=frame_number, label=predicted_label)\n",
    "                preds.predictions.append(pred)\n",
    "        \n",
    "        # Add predictions for this 'k'\n",
    "        result.append(preds)\n",
    "        print(f'Finished inference for k={i}')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_preds(video_id: str, predictions: ListOfPredictionList):\n",
    "    result = None\n",
    "    # Store all probas for each frame_number\n",
    "    frame_proba_dict = {}\n",
    "\n",
    "    # Iterate in PredictionList\n",
    "    for pred_list in predictions:\n",
    "        # Iterate in  Prediction of PredictionList\n",
    "        for pred in pred_list.predictions:\n",
    "            frame = pred.frame\n",
    "            proba = pred.proba\n",
    "\n",
    "            # Add proba\n",
    "            if frame not in frame_proba_dict:\n",
    "                frame_proba_dict[frame] = []\n",
    "            \n",
    "            frame_proba_dict[frame].append(proba)\n",
    "\n",
    "    # Compute the average\n",
    "    result = PredictionList(k=None, video_id=video_id, predictions=[])\n",
    "    for frame, probas in frame_proba_dict.items():\n",
    "        avg_prob = np.mean(probas)\n",
    "        avg_label = 0 if avg_prob < threshold else 1\n",
    "        pred = Prediction(frame=frame, proba=avg_prob, label=avg_label)\n",
    "        # Add to PredictionList\n",
    "        result.predictions.append(pred)\n",
    "\n",
    "    # Sort results by growing frame number\n",
    "    result.predictions = sorted(result.predictions, key=lambda x: x.frame)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_json(avg_preds: PredictionList, file_name: str = 'predictions.json'):\n",
    "    # Initialize an empty dictionary\n",
    "    video_dict = {}\n",
    "    \n",
    "    # Create a list to hold frame and label information\n",
    "    frame_label_list = []\n",
    "    \n",
    "    # Loop through each Prediction object in avg_preds\n",
    "    for prediction in avg_preds.predictions:\n",
    "        # Create a dictionary for each frame\n",
    "        frame_dict = {}\n",
    "        frame_dict[\"frame\"] = prediction.frame\n",
    "        frame_dict[\"label\"] = prediction.label \n",
    "        \n",
    "        # Append to the list\n",
    "        frame_label_list.append(frame_dict)\n",
    "    \n",
    "    # Add the list to the video dictionary with the key as video_id\n",
    "    video_dict[avg_preds.video_id] = frame_label_list\n",
    "    \n",
    "    # Load existing data from the JSON file if it exists\n",
    "    try:\n",
    "        with open(file_name, 'r') as json_file:\n",
    "            existing_data = json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = {}\n",
    "    \n",
    "    # Update the existing data with new video_dict\n",
    "    existing_data.update(video_dict)\n",
    "    \n",
    "    # Save the updated dictionary back to the JSON file\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        print('Saving to json...')\n",
    "        json.dump(existing_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(checkpoint):\n",
    "    for index, row in df.iterrows():\n",
    "        video_id = row['id']\n",
    "        id_int = int(video_id[3:])\n",
    "\n",
    "        print(f'Current id: {id_int}')\n",
    "\n",
    "        if id_int < checkpoint:\n",
    "            video_name = row['directory']\n",
    "            current_vid = os.path.join(video_dir, video_name)\n",
    "            \n",
    "            # Get predictions for each model\n",
    "            predictions: ListOfPredictionList = run_inference(video_id, current_vid)\n",
    "            # Compute the probability of the average model, and get labels from it\n",
    "            avg_preds = compute_avg_preds(video_id=video_id, predictions=predictions)\n",
    "            # Add output to json\n",
    "            append_to_json(avg_preds, 'predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data(checkpoint=108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
