{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Inference for RNN with context"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db3f5e4b23be657"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:56.753411859Z",
     "start_time": "2023-11-30T15:02:56.301845295Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from build.lib.navi.nn.models_using_embeddings import ResetNet50GRU\n",
    "from navi.datasets.frames_embeddings import FramesWithContextDataset\n",
    "from navi.datasets.single_video_with_context import SingleVideoWithContext\n",
    "from navi.datasets.video_with_context import VideoWithContext\n",
    "#from navi.nn.models_using_embeddings import ResetNet50GRU\n",
    "from navi.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def create_dfs(video_csv: str, label_map_path: str) -> dict:\n",
    "    # Dict to store all dataframes\n",
    "    result: dict = {}\n",
    "    mapping_df = pd.read_csv(video_csv)  # df that maps a video id to its folder\n",
    "\n",
    "    # Open json file\n",
    "    with open(label_map_path, 'r') as f:\n",
    "        label_map: pd.DataFrame = json.load(f)\n",
    "\n",
    "        for index, row in mapping_df.iterrows():\n",
    "            video_id = row['id']\n",
    "            if video_id in label_map:\n",
    "                video_data = label_map[video_id]\n",
    "                if isinstance(video_data, list) and all(isinstance(item, dict) for item in video_data):\n",
    "                    video_data_df = pd.DataFrame(video_data)\n",
    "                else:\n",
    "                    raise Exception(f'Invalid data format for video ID: {video_id}')\n",
    "                result[video_id] = video_data_df\n",
    "            else:\n",
    "                raise Exception(f'Could not find the following id in json file: {video_id}')\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.056522670Z",
     "start_time": "2023-11-30T15:02:56.308695477Z"
    }
   },
   "id": "510589cb8fdbafdd"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def get_output_sub_dir(output_dir: str, key: str):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_sub_dir = os.path.join(output_dir, key)\n",
    "    if not os.path.exists(output_sub_dir):\n",
    "        os.makedirs(output_sub_dir)\n",
    "\n",
    "    return output_sub_dir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.056753813Z",
     "start_time": "2023-11-30T15:02:56.348918574Z"
    }
   },
   "id": "d91ce5005c5ed107"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from navi.nn.embeddings_fc import ResNet50FC\n",
    "\n",
    "def get_model(device, model_path, hidden_size, n_layers, fc=True):\n",
    "    if fc:\n",
    "        model = ResNet50FC(\n",
    "            input_size=2048,\n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "    else:\n",
    "        model = ResetNet50GRU(\n",
    "            input_size=2048,\n",
    "            hidden_size=hidden_size,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "    checkpoint = torch.load(model_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.056828584Z",
     "start_time": "2023-11-30T15:02:56.349080089Z"
    }
   },
   "id": "cb7b944308f86e9e"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def get_models(weights_dir: str, nb_models: int=5, verbose=False):\n",
    "    weights = []\n",
    "\n",
    "    for i in range(nb_models):\n",
    "        fold_dir = os.path.join(weights_dir, f\"fold_{i}\")\n",
    "\n",
    "        best_pt_path = os.path.join(fold_dir, 'best.pt')\n",
    "\n",
    "        if os.path.exists(best_pt_path):\n",
    "            weights.append(best_pt_path)\n",
    "        else:\n",
    "            raise Exception(f'Missing weights for fold at: {fold_dir}')\n",
    "\n",
    "    if verbose:\n",
    "        for weight in weights:\n",
    "            print(weight)\n",
    "\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.057202448Z",
     "start_time": "2023-11-30T15:02:56.349168222Z"
    }
   },
   "id": "f66089d633a5c9cb"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def infer(model_paths: list, embeddings_dir: str, test_videos: pd.DataFrame, label_map: pd.DataFrame,\n",
    "          context_size: int, batch_size: int, hidden_size: int, n_layers: int, label_df, model_name='fc') -> dict:\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    print(f'Device: {device}')\n",
    "\n",
    "    #model_nb = 0\n",
    "    transforms = ToTensor()\n",
    "\n",
    "    inference_dataset = FramesWithContextDataset(\n",
    "                    embeddings_dir,\n",
    "                    test_videos,\n",
    "                    label_map,\n",
    "                    transform=transforms,\n",
    "                    context_size=context_size,\n",
    "                )\n",
    "    # Get DataLoader\n",
    "    inference_loader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for model_nb in range(len(model_paths)):\n",
    "        print(f\"Processing model {model_nb}\")\n",
    "        model = get_model(device, model_paths[model_nb], hidden_size, n_layers)\n",
    "\n",
    "        for key in label_df.keys():\n",
    "            label_df[key][f'proba_{model_nb}'] = None\n",
    "    \n",
    "        label_df: dict = run_batches(device, model, inference_loader, label_df, model_nb)\n",
    "\n",
    "        # Run inference\n",
    "        # if model_name == 'fc':\n",
    "        #     label_df: dict = run_batches_fc(device, model, inference_loader, label_df, model_nb)\n",
    "        # else:\n",
    "        #     label_df: dict = run_batches(device, model, inference_loader, label_df, model_nb)\n",
    "\n",
    "    return label_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.057259342Z",
     "start_time": "2023-11-30T15:02:56.396852005Z"
    }
   },
   "id": "8750bb56313f6806"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def run_batches_fc(device, model, inference_loader, label_df, model_nb) -> dict:\n",
    "    compteur = 0\n",
    "    current_df_key = list(label_df.keys())[0]\n",
    "    current_df = label_df[current_df_key]\n",
    "\n",
    "    for batch, (features, labels) in tqdm(enumerate(inference_loader), desc='inference'):\n",
    "        with torch.inference_mode():\n",
    "            features = features.to(device)\n",
    "            features = features[:, -1, :]\n",
    "            logits = model(features).squeeze(dim=-1)\n",
    "            predictions = (logits.sigmoid() > 0.5).long()\n",
    "            predictions = predictions.squeeze()\n",
    "            proba = logits.sigmoid().squeeze()\n",
    "            \n",
    "            if compteur >= len(current_df):\n",
    "                current_df_key = list(label_df.keys())[list(label_df.keys()).index(current_df_key) + 1]\n",
    "                current_df = label_df[current_df_key]\n",
    "                compteur = 0 \n",
    "\n",
    "            # Ajoute la prédiction au DataFrame courant\n",
    "            col_to_write = f'proba_{model_nb}'\n",
    "            current_df.at[compteur, col_to_write] = proba[-1].item()\n",
    "            compteur += 1\n",
    "\n",
    "    return label_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.057314124Z",
     "start_time": "2023-11-30T15:02:56.397021016Z"
    }
   },
   "id": "b251c49fcaf4fec9"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def run_batches(device, model, inference_loader, label_df, model_nb) -> dict:\n",
    "    compteur = 0\n",
    "    current_df_key = list(label_df.keys())[0]\n",
    "    current_df = label_df[current_df_key]\n",
    "\n",
    "    for batch, (features, labels) in tqdm(enumerate(inference_loader), desc='inference'):\n",
    "        with torch.inference_mode():\n",
    "            features = features.to(device)\n",
    "            logits = model(features).squeeze(dim=-1)\n",
    "            predictions = (logits.sigmoid() > 0.5).long()\n",
    "            predictions = predictions.squeeze()\n",
    "            proba = logits.sigmoid().squeeze()\n",
    "            \n",
    "            if compteur >= len(current_df):\n",
    "                current_df_key = list(label_df.keys())[list(label_df.keys()).index(current_df_key) + 1]\n",
    "                current_df = label_df[current_df_key]\n",
    "                compteur = 0 \n",
    "\n",
    "            # Ajoute la prédiction au DataFrame courant\n",
    "            col_to_write = f'proba_{model_nb}'\n",
    "            current_df.at[compteur, col_to_write] = proba[-1].item()\n",
    "            compteur += 1\n",
    "\n",
    "    return label_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.058588955Z",
     "start_time": "2023-11-30T15:02:56.400205679Z"
    }
   },
   "id": "55602214395b6d9f"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "weights_dir=\"/home/rob/Documents/Github/navi_lstm/output/snapshot/v25_fn_resnet/grid_0\"\n",
    "#weights_dir=\"/home/rob/Documents/Github/navi_lstm/notebooks/\"\n",
    "\n",
    "k=5\n",
    "embeddings_dir='/home/rob/Documents/lstm_backup/embeddings_resnet_test' \n",
    "video_csv='/home/rob/Documents/lstm_backup/maps/mapping_test.csv'\n",
    "label_map_path='/home/rob/Documents/lstm_backup/labels/ground_truth_testset.json' \n",
    "context_size=5\n",
    "batch_size=1\n",
    "hidden_size=32\n",
    "n_layers=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:02:57.058694769Z",
     "start_time": "2023-11-30T15:02:56.440696053Z"
    }
   },
   "id": "198ebd408285fb9c"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Processing Dataframes\n",
      "Collecting Models\n",
      "Device: cpu\n",
      "Loading embeddings...\n",
      "Loading targets...\n",
      "Indexing frames...\n",
      "Dataset loaded.\n",
      "Processing model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 24124it [00:05, 4563.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 24124it [00:05, 4042.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 24124it [00:06, 3931.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 24124it [00:04, 5013.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference: 24124it [00:04, 5124.24it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/home/rob/Documents/Github/navi_lstm/output/inference'\n",
    "output_sub_dir: dict = {}\n",
    "dataframe_dict = create_dfs(video_csv, label_map_path)\n",
    "test_videos = pd.read_csv(video_csv)\n",
    "\n",
    "print('Pre Processing Dataframes')\n",
    "for key, dataframe in dataframe_dict.items():\n",
    "    try:\n",
    "        # pre_process_df(dataframe, k)\n",
    "        # dataframe_dict[key] = dataframe\n",
    "        # stores the path to save output\n",
    "        output_sub_dir[key] = get_output_sub_dir(output_dir, key)\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Error during Pre Processing of Dataframes for key: {key} | \\nException: {e}')\n",
    "\n",
    "video_ids = sorted(dataframe_dict.keys())\n",
    "\n",
    "print('Collecting Models')\n",
    "model_paths = get_models(weights_dir, nb_models=k)\n",
    "with open(label_map_path, 'rb') as file:\n",
    "    label_map = json.load(file)\n",
    "\n",
    "results_df = infer(model_paths=model_paths,\n",
    "                          embeddings_dir=embeddings_dir, test_videos=test_videos, label_map=label_map,\n",
    "                          context_size=context_size, batch_size=batch_size, hidden_size=hidden_size,\n",
    "                          n_layers=n_layers, label_df=dataframe_dict)\n",
    "\n",
    "#verify_matching_keys(dataframe_dict, df_dict_one_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.545734098Z",
     "start_time": "2023-11-30T15:02:56.440821186Z"
    }
   },
   "id": "2f486e8a00b944bb"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['id_x1', 'id_x2', 'id_x3', 'id_x4', 'id_x5'])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.545871124Z",
     "start_time": "2023-11-30T15:03:23.544807116Z"
    }
   },
   "id": "ae3eccbf71e428ea"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "      frame  label   proba_0   proba_1   proba_2   proba_3   proba_4\n0         0      0  0.695349  0.604026  0.693878   0.78162  0.740069\n1         1      0  0.588238  0.511964  0.534744  0.683259  0.580529\n2         2      0  0.687131  0.534376   0.54728  0.753236  0.654674\n3         3      0  0.630827  0.477583  0.550627  0.702151  0.681208\n4         4      0  0.587383    0.4571  0.498581  0.709815  0.602283\n...     ...    ...       ...       ...       ...       ...       ...\n1365   1365      0  0.635369  0.607195  0.676168  0.814261  0.776786\n1366   1366      0  0.538782  0.545471  0.646625  0.603083  0.719442\n1367   1367      0  0.684252  0.669105  0.774884  0.732205  0.788397\n1368   1368      0  0.652685  0.518505  0.777622  0.758464  0.867524\n1369   1369      0  0.729796  0.628923  0.841423  0.825683  0.932623\n\n[1370 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>label</th>\n      <th>proba_0</th>\n      <th>proba_1</th>\n      <th>proba_2</th>\n      <th>proba_3</th>\n      <th>proba_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.695349</td>\n      <td>0.604026</td>\n      <td>0.693878</td>\n      <td>0.78162</td>\n      <td>0.740069</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.588238</td>\n      <td>0.511964</td>\n      <td>0.534744</td>\n      <td>0.683259</td>\n      <td>0.580529</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0.687131</td>\n      <td>0.534376</td>\n      <td>0.54728</td>\n      <td>0.753236</td>\n      <td>0.654674</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0.630827</td>\n      <td>0.477583</td>\n      <td>0.550627</td>\n      <td>0.702151</td>\n      <td>0.681208</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0.587383</td>\n      <td>0.4571</td>\n      <td>0.498581</td>\n      <td>0.709815</td>\n      <td>0.602283</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1365</th>\n      <td>1365</td>\n      <td>0</td>\n      <td>0.635369</td>\n      <td>0.607195</td>\n      <td>0.676168</td>\n      <td>0.814261</td>\n      <td>0.776786</td>\n    </tr>\n    <tr>\n      <th>1366</th>\n      <td>1366</td>\n      <td>0</td>\n      <td>0.538782</td>\n      <td>0.545471</td>\n      <td>0.646625</td>\n      <td>0.603083</td>\n      <td>0.719442</td>\n    </tr>\n    <tr>\n      <th>1367</th>\n      <td>1367</td>\n      <td>0</td>\n      <td>0.684252</td>\n      <td>0.669105</td>\n      <td>0.774884</td>\n      <td>0.732205</td>\n      <td>0.788397</td>\n    </tr>\n    <tr>\n      <th>1368</th>\n      <td>1368</td>\n      <td>0</td>\n      <td>0.652685</td>\n      <td>0.518505</td>\n      <td>0.777622</td>\n      <td>0.758464</td>\n      <td>0.867524</td>\n    </tr>\n    <tr>\n      <th>1369</th>\n      <td>1369</td>\n      <td>0</td>\n      <td>0.729796</td>\n      <td>0.628923</td>\n      <td>0.841423</td>\n      <td>0.825683</td>\n      <td>0.932623</td>\n    </tr>\n  </tbody>\n</table>\n<p>1370 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['id_x1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.546474546Z",
     "start_time": "2023-11-30T15:03:23.545041872Z"
    }
   },
   "id": "d77f5d2798c29f97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get average"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "220ff3d067ee845e"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def average_prediction(frame_df, k_range):\n",
    "    prob_cols = [f'proba_{k}' for k in k_range]\n",
    "    for col in prob_cols:\n",
    "        if col not in frame_df.columns:\n",
    "            raise ValueError(f\"Missing column {col} in DataFrame\")\n",
    "\n",
    "    # Average probability for each row (axis=1 -> column)\n",
    "    frame_df['proba_avg'] = frame_df[prob_cols].mean(axis=1)\n",
    "    # Average prediction\n",
    "    frame_df['pred_avg'] = np.where(frame_df['proba_avg'] > 0.5, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.614429102Z",
     "start_time": "2023-11-30T15:03:23.545219598Z"
    }
   },
   "id": "ac590c7260c87fa6"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "for key, dataframe in results_df.items():\n",
    "    average_prediction(dataframe, range(5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.641538535Z",
     "start_time": "2023-11-30T15:03:23.588893208Z"
    }
   },
   "id": "8e0939af713531fd"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "      frame  label   proba_0   proba_1   proba_2   proba_3   proba_4  \\\n0         0      0  0.695349  0.604026  0.693878   0.78162  0.740069   \n1         1      0  0.588238  0.511964  0.534744  0.683259  0.580529   \n2         2      0  0.687131  0.534376   0.54728  0.753236  0.654674   \n3         3      0  0.630827  0.477583  0.550627  0.702151  0.681208   \n4         4      0  0.587383    0.4571  0.498581  0.709815  0.602283   \n...     ...    ...       ...       ...       ...       ...       ...   \n1365   1365      0  0.635369  0.607195  0.676168  0.814261  0.776786   \n1366   1366      0  0.538782  0.545471  0.646625  0.603083  0.719442   \n1367   1367      0  0.684252  0.669105  0.774884  0.732205  0.788397   \n1368   1368      0  0.652685  0.518505  0.777622  0.758464  0.867524   \n1369   1369      0  0.729796  0.628923  0.841423  0.825683  0.932623   \n\n     proba_avg  pred_avg  \n0     0.702988         1  \n1     0.579747         1  \n2      0.63534         1  \n3     0.608479         1  \n4     0.571032         1  \n...        ...       ...  \n1365  0.701956         1  \n1366  0.610681         1  \n1367  0.729769         1  \n1368   0.71496         1  \n1369   0.79169         1  \n\n[1370 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>label</th>\n      <th>proba_0</th>\n      <th>proba_1</th>\n      <th>proba_2</th>\n      <th>proba_3</th>\n      <th>proba_4</th>\n      <th>proba_avg</th>\n      <th>pred_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.695349</td>\n      <td>0.604026</td>\n      <td>0.693878</td>\n      <td>0.78162</td>\n      <td>0.740069</td>\n      <td>0.702988</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.588238</td>\n      <td>0.511964</td>\n      <td>0.534744</td>\n      <td>0.683259</td>\n      <td>0.580529</td>\n      <td>0.579747</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0.687131</td>\n      <td>0.534376</td>\n      <td>0.54728</td>\n      <td>0.753236</td>\n      <td>0.654674</td>\n      <td>0.63534</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0.630827</td>\n      <td>0.477583</td>\n      <td>0.550627</td>\n      <td>0.702151</td>\n      <td>0.681208</td>\n      <td>0.608479</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0.587383</td>\n      <td>0.4571</td>\n      <td>0.498581</td>\n      <td>0.709815</td>\n      <td>0.602283</td>\n      <td>0.571032</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1365</th>\n      <td>1365</td>\n      <td>0</td>\n      <td>0.635369</td>\n      <td>0.607195</td>\n      <td>0.676168</td>\n      <td>0.814261</td>\n      <td>0.776786</td>\n      <td>0.701956</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1366</th>\n      <td>1366</td>\n      <td>0</td>\n      <td>0.538782</td>\n      <td>0.545471</td>\n      <td>0.646625</td>\n      <td>0.603083</td>\n      <td>0.719442</td>\n      <td>0.610681</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1367</th>\n      <td>1367</td>\n      <td>0</td>\n      <td>0.684252</td>\n      <td>0.669105</td>\n      <td>0.774884</td>\n      <td>0.732205</td>\n      <td>0.788397</td>\n      <td>0.729769</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1368</th>\n      <td>1368</td>\n      <td>0</td>\n      <td>0.652685</td>\n      <td>0.518505</td>\n      <td>0.777622</td>\n      <td>0.758464</td>\n      <td>0.867524</td>\n      <td>0.71496</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1369</th>\n      <td>1369</td>\n      <td>0</td>\n      <td>0.729796</td>\n      <td>0.628923</td>\n      <td>0.841423</td>\n      <td>0.825683</td>\n      <td>0.932623</td>\n      <td>0.79169</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1370 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['id_x1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.642159921Z",
     "start_time": "2023-11-30T15:03:23.589016369Z"
    }
   },
   "id": "bc3dd9b8c3f589ce"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "for key, df in results_df.items():\n",
    "    output_file_name = f\"{key}_output.csv\"\n",
    "    df.to_csv(output_file_name, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.784479528Z",
     "start_time": "2023-11-30T15:03:23.590277697Z"
    }
   },
   "id": "f8048f373fe2e144"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def plot_labels_and_predictions(df, plot_title, output_dir):\n",
    "    df = df.sort_values(by='frame')\n",
    "\n",
    "    true_labels = df['label'].tolist()\n",
    "    model_predictions = df['pred_0'].tolist()\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 4))\n",
    "    tick_interval = 250\n",
    "    frame_numbers = df['frame'].tolist()\n",
    "    xticks = np.arange(min(frame_numbers), max(frame_numbers) + tick_interval, tick_interval)\n",
    "    xticklabels = np.arange(min(frame_numbers), max(frame_numbers) + tick_interval, tick_interval).astype(int)\n",
    "\n",
    "    for i, label in enumerate(true_labels):\n",
    "        color = 'white' if label == 0 else 'dimgray'\n",
    "        axs[0].axhspan(0, 1, xmin=i / len(frame_numbers), xmax=(i + 1) / len(frame_numbers), facecolor=color)\n",
    "\n",
    "    axs[0].set_yticks([])\n",
    "    axs[0].set_title('True Labels', fontsize=14, fontweight='bold')\n",
    "    axs[0].set_xticks(xticks / max(frame_numbers))\n",
    "    axs[0].set_xticklabels(xticklabels)\n",
    "\n",
    "    for i, label in enumerate(model_predictions):\n",
    "        color = 'white' if label == 0 else 'darkgray'\n",
    "        axs[1].axhspan(0, 1, xmin=i / len(frame_numbers), xmax=(i + 1) / len(frame_numbers), facecolor=color)\n",
    "\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_title(plot_title, fontsize=14, fontweight='bold')\n",
    "    axs[1].set_xticks(xticks / max(frame_numbers))\n",
    "    axs[1].set_xticklabels(xticklabels)\n",
    "\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    label_rectangle_path = os.path.join(output_dir, 'label_vis.png')\n",
    "    plt.savefig(label_rectangle_path)\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.829059681Z",
     "start_time": "2023-11-30T15:03:23.787744986Z"
    }
   },
   "id": "48519c47495bd199"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# def evaluate_models(df):\n",
    "#     # Initialize DataFrame\n",
    "#     results_df = pd.DataFrame(\n",
    "#         columns=['average', 'balanced_accuracy', 'accuracy', 'precision', 'recall', 'f1_score', 'correct_no', 'incorrect_no', 'correct_spur',\n",
    "#                  'incorrect_spur'])\n",
    "# \n",
    "#     # Extract true labels from df and filter None values\n",
    "#     true_labels_k = [label for label in df['label'].tolist() if label is not None]\n",
    "# \n",
    "#     for col in df.columns:\n",
    "#         if col.startswith('pred_'):\n",
    "#             # Filter None values\n",
    "#             model_labels_k = [label for label in df[col].tolist() if label is not None]\n",
    "# \n",
    "#             if len(true_labels_k) != len(model_labels_k):\n",
    "#                 print(f\"Skipping evaluation for {col} due to inconsistent label lengths after filtering None values.\")\n",
    "#                 continue\n",
    "# \n",
    "#             model_name = col.split('_')[1]\n",
    "#             results_df.loc[len(results_df)] = calculate_metrics(true_labels_k, model_labels_k, model_name)\n",
    "# \n",
    "#     return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.829240162Z",
     "start_time": "2023-11-30T15:03:23.828904674Z"
    }
   },
   "id": "45e27240e1eb08ae"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def evaluate_models(df):\n",
    "    # Initialize DataFrame\n",
    "    results_df = pd.DataFrame(columns=['model', 'balanced_accuracy', 'accuracy', 'precision', 'recall', 'f1_score', 'correct_no', 'incorrect_no', 'correct_spur', 'incorrect_spur'])\n",
    "\n",
    "    # Extract true labels from df and filter None values\n",
    "    true_labels_k = [label for label in df['label'].tolist() if label is not None]\n",
    "    model_labels_k = [label for label in df['pred_avg'].tolist() if label is not None]\n",
    "\n",
    "    if len(true_labels_k) != len(model_labels_k):\n",
    "        print(\"Skipping evaluation due to inconsistent label lengths after filtering None values.\")\n",
    "    else:\n",
    "        results_df.loc[len(results_df)] = calculate_metrics(true_labels_k, model_labels_k, 'pred_avg')\n",
    "\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.872917105Z",
     "start_time": "2023-11-30T15:03:23.829012656Z"
    }
   },
   "id": "7d3aec26a1ed39ac"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, pred_labels, label_name):\n",
    "    # Calculate metrics\n",
    "    accuracy = round(accuracy_score(true_labels, pred_labels) * 100, 2)  # TP + TN / n\n",
    "    balanced_accuracy = round(balanced_accuracy_score(true_labels, pred_labels) * 100, 2)  \n",
    "\n",
    "    precision = round(precision_score(true_labels, pred_labels, pos_label=1, average='binary') * 100,\n",
    "                      2)  # TP / TP + FP\n",
    "    recall = round(recall_score(true_labels, pred_labels, pos_label=1, average='binary') * 100, 2)  # TP / TP + FN\n",
    "    f1 = round(f1_score(true_labels, pred_labels, pos_label=1, average='binary') * 100,\n",
    "               2)  # 2 * [(recall * precision) / (recall + precision)]\n",
    "\n",
    "    # Calculate percentages\n",
    "    correct_no = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true == pred and true == 0]) / true_labels.count(\n",
    "            0) * 100, 2)\n",
    "    incorrect_no = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true != pred and true == 0]) / true_labels.count(\n",
    "            0) * 100, 2)\n",
    "    correct_spur = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true == pred and true == 1]) / true_labels.count(\n",
    "            1) * 100, 2)\n",
    "    incorrect_spur = round(\n",
    "        sum([1 for true, pred in zip(true_labels, pred_labels) if true != pred and true == 1]) / true_labels.count(\n",
    "            1) * 100, 2)\n",
    "\n",
    "    return [label_name, balanced_accuracy, accuracy, precision, recall, f1, correct_no, incorrect_no, correct_spur, incorrect_spur]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.873061470Z",
     "start_time": "2023-11-30T15:03:23.872797254Z"
    }
   },
   "id": "a9eaf5127f11248e"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "df_id_x1 = results_df['id_x1']  \n",
    "\n",
    "results_frame_df = evaluate_models(df_id_x1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.887824628Z",
     "start_time": "2023-11-30T15:03:23.872896632Z"
    }
   },
   "id": "f1209b80ff7e99eb"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "      model  balanced_accuracy  accuracy  precision  recall  f1_score  \\\n0  pred_avg              61.97     52.48      41.95   93.53     57.92   \n\n   correct_no  incorrect_no  correct_spur  incorrect_spur  \n0       30.42         69.58         93.53            6.47  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>balanced_accuracy</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_no</th>\n      <th>incorrect_no</th>\n      <th>correct_spur</th>\n      <th>incorrect_spur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pred_avg</td>\n      <td>61.97</td>\n      <td>52.48</td>\n      <td>41.95</td>\n      <td>93.53</td>\n      <td>57.92</td>\n      <td>30.42</td>\n      <td>69.58</td>\n      <td>93.53</td>\n      <td>6.47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_frame_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.929290345Z",
     "start_time": "2023-11-30T15:03:23.885529937Z"
    }
   },
   "id": "5b85536948632d50"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "      model  balanced_accuracy  accuracy  precision  recall  f1_score  \\\n0  pred_avg              77.76     81.15      45.28   72.73     55.81   \n\n   correct_no  incorrect_no  correct_spur  incorrect_spur  \n0        82.8          17.2         72.73           27.27  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>balanced_accuracy</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_no</th>\n      <th>incorrect_no</th>\n      <th>correct_spur</th>\n      <th>incorrect_spur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pred_avg</td>\n      <td>77.76</td>\n      <td>81.15</td>\n      <td>45.28</td>\n      <td>72.73</td>\n      <td>55.81</td>\n      <td>82.8</td>\n      <td>17.2</td>\n      <td>72.73</td>\n      <td>27.27</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id_x = results_df['id_x2']  \n",
    "results_frame_df = evaluate_models(df_id_x)\n",
    "results_frame_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:23.929553187Z",
     "start_time": "2023-11-30T15:03:23.928791085Z"
    }
   },
   "id": "f8cc9a38aec9925"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "      model  balanced_accuracy  accuracy  precision  recall  f1_score  \\\n0  pred_avg               83.6     87.26      42.35   79.03     55.15   \n\n   correct_no  incorrect_no  correct_spur  incorrect_spur  \n0       88.17         11.83         79.03           20.97  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>balanced_accuracy</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_no</th>\n      <th>incorrect_no</th>\n      <th>correct_spur</th>\n      <th>incorrect_spur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pred_avg</td>\n      <td>83.6</td>\n      <td>87.26</td>\n      <td>42.35</td>\n      <td>79.03</td>\n      <td>55.15</td>\n      <td>88.17</td>\n      <td>11.83</td>\n      <td>79.03</td>\n      <td>20.97</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id_x = results_df['id_x3']  \n",
    "results_frame_df = evaluate_models(df_id_x)\n",
    "results_frame_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:24.039711157Z",
     "start_time": "2023-11-30T15:03:23.972797641Z"
    }
   },
   "id": "204294eb3b52e567"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "      model  balanced_accuracy  accuracy  precision  recall  f1_score  \\\n0  pred_avg              80.59     78.54      46.13   83.88     59.53   \n\n   correct_no  incorrect_no  correct_spur  incorrect_spur  \n0        77.3          22.7         83.88           16.12  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>balanced_accuracy</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_no</th>\n      <th>incorrect_no</th>\n      <th>correct_spur</th>\n      <th>incorrect_spur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pred_avg</td>\n      <td>80.59</td>\n      <td>78.54</td>\n      <td>46.13</td>\n      <td>83.88</td>\n      <td>59.53</td>\n      <td>77.3</td>\n      <td>22.7</td>\n      <td>83.88</td>\n      <td>16.12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id_x = results_df['id_x4']  \n",
    "results_frame_df = evaluate_models(df_id_x)\n",
    "results_frame_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:24.101763776Z",
     "start_time": "2023-11-30T15:03:23.972916895Z"
    }
   },
   "id": "e3b0c93741874f38"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "      model  balanced_accuracy  accuracy  precision  recall  f1_score  \\\n0  pred_avg              90.41     91.91      80.05   87.57     83.64   \n\n   correct_no  incorrect_no  correct_spur  incorrect_spur  \n0       93.25          6.75         87.57           12.43  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>balanced_accuracy</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_no</th>\n      <th>incorrect_no</th>\n      <th>correct_spur</th>\n      <th>incorrect_spur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pred_avg</td>\n      <td>90.41</td>\n      <td>91.91</td>\n      <td>80.05</td>\n      <td>87.57</td>\n      <td>83.64</td>\n      <td>93.25</td>\n      <td>6.75</td>\n      <td>87.57</td>\n      <td>12.43</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id_x = results_df['id_x5']  \n",
    "results_frame_df = evaluate_models(df_id_x)\n",
    "results_frame_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:24.103116156Z",
     "start_time": "2023-11-30T15:03:24.003295665Z"
    }
   },
   "id": "6c5d634970349d25"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pred_0'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/Documents/Github/navi_lstm/.env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'pred_0'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[64], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m plot_title \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredictions for id_x1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfold_0\u001B[39m\u001B[38;5;124m\"\u001B[39m \n\u001B[0;32m----> 3\u001B[0m \u001B[43mplot_labels_and_predictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_id_x1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_title\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[54], line 5\u001B[0m, in \u001B[0;36mplot_labels_and_predictions\u001B[0;34m(df, plot_title, output_dir)\u001B[0m\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mframe\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m true_labels \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m----> 5\u001B[0m model_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpred_0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      7\u001B[0m fig, axs \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[1;32m      8\u001B[0m tick_interval \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m250\u001B[39m\n",
      "File \u001B[0;32m~/Documents/Github/navi_lstm/.env/lib/python3.11/site-packages/pandas/core/frame.py:3896\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3896\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3898\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/Documents/Github/navi_lstm/.env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3795\u001B[0m     ):\n\u001B[1;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'pred_0'"
     ]
    }
   ],
   "source": [
    "plot_title = \"Predictions for id_x1\"\n",
    "output_dir = \"fold_0\" \n",
    "plot_labels_and_predictions(df_id_x1, plot_title, output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T15:03:24.240941077Z",
     "start_time": "2023-11-30T15:03:24.037708863Z"
    }
   },
   "id": "9c1893808e70078d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:03:24.128789403Z"
    }
   },
   "id": "a24201ebfb767d82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
